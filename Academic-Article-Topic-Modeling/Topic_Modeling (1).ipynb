{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "cvkrGNoGvlQx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWnfW2ACQSMc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "doc_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/My Folder/topic_model_train_new.csv')\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# Load stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load punk_tab\n",
        "import string # to remove punctuations\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load SpaCy to use its Lemmatization function\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable = ['ner','parser'])\n",
        "\n",
        "# Define additional stopwords common in the pyLDAvis visualization\n",
        "custom_stopwords = set([\n",
        "    \"author\", \"learn\", \"description\", \"method\", \"result\",\n",
        "    \"paper\", \"study\", \"analysis\", \"research\", \"use\", \"model\",\n",
        "    \"approach\", \"different\", \"propose\", \"find\", \"work\", \"differ\",\n",
        "    \"new\", \"set\", \"paper\", \"show\", \"information\", \"process\", \"present\",\n",
        "    \"process\", \"approach\", \"problem\", \"gross\"\n",
        "])\n",
        "\n",
        "# Merge with NLTK's stopwords\n",
        "all_stopwords = set(stopwords.words('english')).union(custom_stopwords)\n",
        "\n",
        "\n",
        "def my_preprocessing(text):\n",
        "  text_processed = []\n",
        "  #step 0: remove the extra line separaters\n",
        "  text_one_line = text.replace('\\n',' ')\n",
        "  #step 1: lower case\n",
        "  text_lower = text_one_line.lower()\n",
        "  #step 2: remove math expressions and numbers\n",
        "  text_wo_math = re.sub(r'\\$.*?\\$', '', text_lower)\n",
        "  text_wo_numbers = re.sub(r'\\d+', '', text_wo_math)\n",
        "  #step 3: remove stopwords and punctuations\n",
        "  tokens = word_tokenize(text_wo_numbers)\n",
        "  tokens_processed = []\n",
        "  for token in tokens:\n",
        "    if (token not in string.punctuation) and (token not in all_stopwords):\n",
        "      tokens_processed.append(token)\n",
        "  #step 4: lemmatization using SpaCy and also remove short words\n",
        "  text_processed = ' '.join(tokens_processed)\n",
        "  allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
        "  doc = nlp(text_processed)\n",
        "  token_lemma = [token.lemma_ for token in doc if (token.pos_ in allowed_postags) and (len(token) > 2)]\n",
        "  text_processed = ' '.join(token_lemma)\n",
        "  return text_processed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_corpus = doc_df['ABSTRACT'].sample(5)\n",
        "\n",
        "# do your test here\n",
        "for text in sample_corpus:\n",
        "  print(text)\n",
        "  print(my_preprocessing(text))\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "lf1QJPCUSQ6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA Model"
      ],
      "metadata": {
        "id": "QI1DwJ6Fv4I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy --upgrade\n",
        "!pip install gensim --upgrade --force-reinstall\n",
        "\n",
        "# Use TF-IDF vectorizer to turn abstracts into vectors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "my_vectorizer = TfidfVectorizer(preprocessor=my_preprocessing, max_features = 5000)\n",
        "abstract_vectorized = my_vectorizer.fit_transform(doc_df['ABSTRACT'])\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim import corpora\n",
        "\n",
        "coherence_scores = {}\n",
        "\n",
        "# Try different numbers of topics\n",
        "for num_topics in [5, 7, 9, 11]:  # Try different values for n_components\n",
        "    print(f\"Training LDA with {num_topics} topics...\")\n",
        "\n",
        "    lda_model = LatentDirichletAllocation(n_components=num_topics, # Number of topics\n",
        "                                    doc_topic_prior = None, # Default is 1/n_documents\n",
        "                                    topic_word_prior = None, # Default is 1/n_documents\n",
        "                                    learning_method='online',  # Decide how often the training will update the model. Will be faster to do it \"online\" vs \"batch\"\n",
        "                                    random_state= 42,\n",
        "                                    max_iter=10) # The number of epoches for the training (how many times you wlll go through the entire corpus)\n",
        "\n",
        "    lda_top=lda_model.fit_transform(abstract_vectorized)\n",
        "\n",
        "        # Function to extract topic words from Scikit-Learn LDA\n",
        "    def get_sklearn_topics(model, feature_names, n_top_words=10):\n",
        "        topics = []\n",
        "        for topic_idx, topic in enumerate(model.components_):\n",
        "            top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
        "            topics.append(top_words)\n",
        "        return topics\n",
        "\n",
        "    # Get feature names and extract topics\n",
        "    feature_names = my_vectorizer.get_feature_names_out()\n",
        "    sklearn_topics = get_sklearn_topics(lda_model, feature_names)\n",
        "\n",
        "    # Convert preprocessed text data into tokenized form\n",
        "    doc_df['Processed'] = doc_df['ABSTRACT'].apply(lambda x: my_preprocessing(x).split())  # Convert preprocessed text into list of words\n",
        "    dictionary = corpora.Dictionary(doc_df['Processed'])\n",
        "    corpus = [dictionary.doc2bow(text) for text in doc_df['Processed']]\n",
        "\n",
        "    coherence_model_sklearn = CoherenceModel(topics=sklearn_topics, texts=doc_df['Processed'], dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_sklearn.get_coherence()\n",
        "    coherence_scores[num_topics] = coherence_score\n",
        "\n",
        "    print(f\"Coherence Score for {num_topics} topics: {coherence_score}\")\n",
        "\n",
        "print(\"Final Coherence Scores:\")\n",
        "for k, v in coherence_scores.items():\n",
        "    print(f\"{k} Topics: {v}\")\n"
      ],
      "metadata": {
        "id": "XXoaCTvCSZpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LDA model with the best number of topics (5)\n",
        "best_num_topics = 5  # Since 5 had the highest coherence score\n",
        "\n",
        "lda_model_best = LatentDirichletAllocation(n_components=best_num_topics,\n",
        "                                           learning_method='online',\n",
        "                                           random_state=42,\n",
        "                                           max_iter=10)\n",
        "\n",
        "# Fit the model with TF-IDF matrix\n",
        "lda_top_best = lda_model_best.fit_transform(abstract_vectorized)"
      ],
      "metadata": {
        "id": "tjXIoMrjxF9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sklearn_topics(model, feature_names, n_top_words=10):\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
        "        topics.append(top_words)\n",
        "    return topics\n",
        "\n",
        "# Get feature names and extract topics\n",
        "feature_names = my_vectorizer.get_feature_names_out()\n",
        "sklearn_topics = get_sklearn_topics(lda_model_best, feature_names)\n",
        "\n",
        "perplexity_score = lda_model_best.perplexity(abstract_vectorized)\n",
        "\n",
        "print(f\"Scikit-Learn LDA Perplexity Score: {perplexity_score}\")"
      ],
      "metadata": {
        "id": "nGxzv950onag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the top 10 word tokens in each topic\n",
        "vocab = my_vectorizer.get_feature_names_out()\n",
        "for i, comp in enumerate(lda_model_best.components_):\n",
        "    terms_comp = zip(vocab, comp)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:10]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    for t in sorted_terms:\n",
        "        print(t[0],end=\" \")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "BjTwIffowT6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model as sklearnvis\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis_data = sklearnvis.prepare(lda_model_best, abstract_vectorized, my_vectorizer)\n",
        "pyLDAvis.display(vis_data)"
      ],
      "metadata": {
        "id": "ahhK5tYqx6fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing of Embedding + Clustering Techniques"
      ],
      "metadata": {
        "id": "4EXDtotRv-Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample df\n",
        "sample_df = doc_df.sample(2000, ignore_index = True)\n",
        "sample_docs = sample_df['ABSTRACT'].tolist()\n",
        "\n",
        "docs = sample_docs\n",
        "\n",
        "# Model df\n",
        "model_docs = doc_df['ABSTRACT'].tolist()"
      ],
      "metadata": {
        "id": "aqv3OAwsE3bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doc2Vec"
      ],
      "metadata": {
        "id": "jDgWP99LwduN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tagged_docs = []\n",
        "for i,doc in enumerate(docs):\n",
        "  tagged_docs.append(TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]))\n",
        "\n",
        "# train the Doc2vec model\n",
        "model = Doc2Vec(vector_size=20,\n",
        "                min_count=2,\n",
        "                epochs=50)\n",
        "model.build_vocab(tagged_docs)\n",
        "model.train(tagged_docs,\n",
        "            total_examples=model.corpus_count,\n",
        "            epochs=model.epochs)\n",
        "\n",
        "# get the document vectors\n",
        "document_vectors = [model.infer_vector(word_tokenize(my_preprocessing(doc))) for doc in docs]"
      ],
      "metadata": {
        "id": "OPC069G4tDGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as sc\n",
        "\n",
        "sc.dendrogram(sc.linkage(document_vectors, method='ward'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ffXzw0g-12BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "for k in range(5, 10):  # Try different cluster numbers\n",
        "    my_clustering = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
        "    cluster_labels = my_clustering.fit_predict(document_vectors)\n",
        "    score = silhouette_score(document_vectors, cluster_labels)\n",
        "    print(f\"Clusters: {k}, Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "ZP44Hx8KxLh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Try different cluster numbers with K-Means\n",
        "for k in range(5, 10):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels_kmeans = kmeans.fit_predict(document_vectors)\n",
        "    score = silhouette_score(document_vectors, cluster_labels_kmeans)\n",
        "    print(f\"K-Means Clusters: {k}, Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "um1PtfZixTJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SBERT"
      ],
      "metadata": {
        "id": "0Ggj5oQawhpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "sbert_embeddings = sbert_model.encode(docs)"
      ],
      "metadata": {
        "id": "qUubEViLIw_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as sc\n",
        "\n",
        "sc.dendrogram(sc.linkage(sbert_embeddings, method='ward'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "itHxA4zFI85Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "for k in range(5, 10):  # Try different cluster numbers\n",
        "    my_clustering = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
        "    cluster_labels = my_clustering.fit_predict(sbert_embeddings)\n",
        "    score = silhouette_score(sbert_embeddings, cluster_labels)\n",
        "    print(f\"Clusters: {k}, Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "IFM4FGKXX-Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(5, 10):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels_kmeans = kmeans.fit_predict(sbert_embeddings)\n",
        "    score = silhouette_score(sbert_embeddings, cluster_labels_kmeans)\n",
        "    print(f\"K-Means Clusters: {k}, Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "T_TP2vTVbcCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Universal Sentence Embedding"
      ],
      "metadata": {
        "id": "THwruvIYxh5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "use_embeddings = use_model(docs)\n",
        "\n",
        "sc.dendrogram(sc.linkage(use_embeddings, method='ward'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wIi1XWulXvMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(5, 10):  # Try different cluster numbers\n",
        "    my_clustering = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
        "    cluster_labels = my_clustering.fit_predict(use_embeddings)\n",
        "    score = silhouette_score(use_embeddings, cluster_labels)\n",
        "    print(f\"Clusters: {k}, Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "q_eL6kMsijIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(5, 10):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels_kmeans = kmeans.fit_predict(use_embeddings)\n",
        "    score = silhouette_score(use_embeddings, cluster_labels_kmeans)\n",
        "    print(f\"K-Means Clusters: {k}, Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "scPdpVLrimW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Build of Doc2Vec + K Means"
      ],
      "metadata": {
        "id": "PLUfCnji4x6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_tagged_docs = [\n",
        "    TaggedDocument(words=my_preprocessing(doc).split(), tags=[str(i)]) for i, doc in enumerate(doc_df['ABSTRACT'])\n",
        "]\n",
        "\n",
        "doc_model = Doc2Vec(vector_size=20, min_count=2, epochs=50, workers=4)\n",
        "\n",
        "doc_model.build_vocab(model_tagged_docs)\n",
        "\n",
        "doc_model.train(model_tagged_docs, total_examples=doc_model.corpus_count, epochs=doc_model.epochs)\n",
        "\n",
        "model_document_vectors = np.array([doc_model.infer_vector(my_preprocessing(doc).split()) for doc in doc_df['ABSTRACT']])\n"
      ],
      "metadata": {
        "id": "kAGrwY2r7FaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
        "cluster_labels_kmeans = kmeans.fit_predict(model_document_vectors)\n",
        "\n",
        "# Compute Silhouette Score\n",
        "silhouette_kmeans_doc2vec = silhouette_score(model_document_vectors, cluster_labels_kmeans)\n",
        "print(f\"Final K-Means Doc2Vec Silhouette Score: {silhouette_kmeans_doc2vec:.4f}\")\n",
        "\n",
        "\n",
        "doc_df['Cluster'] = cluster_labels_kmeans\n",
        "\n",
        "# Show some results\n",
        "print(doc_df[['TITLE', 'Cluster']].head())"
      ],
      "metadata": {
        "id": "janDK_QZ42tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "model_docs = doc_df['ABSTRACT'].tolist()\n",
        "\n",
        "# Convert abstracts into TF-IDF representation\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "tfidf_matrix = vectorizer.fit_transform(model_docs)\n",
        "\n",
        "# Extract feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Function to get top words for each cluster\n",
        "def get_top_words_per_cluster(tfidf_matrix, cluster_labels, feature_names, n_words=10):\n",
        "    clusters = np.unique(cluster_labels)\n",
        "    top_words = []\n",
        "\n",
        "    for cluster in clusters:\n",
        "        # Get indices of documents in the current cluster\n",
        "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
        "\n",
        "        # Extract only the documents belonging to this cluster\n",
        "        cluster_docs = tfidf_matrix[cluster_indices]\n",
        "\n",
        "        # Compute average TF-IDF score for each word\n",
        "        cluster_mean_tfidf = np.asarray(cluster_docs.mean(axis=0)).flatten()\n",
        "\n",
        "        # Get top words for the cluster\n",
        "        top_indices = cluster_mean_tfidf.argsort()[-n_words:][::-1]\n",
        "        top_words.append([feature_names[i] for i in top_indices])\n",
        "\n",
        "    return top_words\n",
        "\n",
        "# Extract top words for each cluster\n",
        "top_words_per_cluster = get_top_words_per_cluster(tfidf_matrix, cluster_labels_kmeans, feature_names)\n",
        "\n",
        "# Print top words for each cluster\n",
        "for i, words in enumerate(top_words_per_cluster):\n",
        "    print(f\"Cluster {i}: {', '.join(words)}\")"
      ],
      "metadata": {
        "id": "0jSzhV9J8P7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count articles per cluster\n",
        "cluster_counts = doc_df['Cluster'].value_counts()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "cluster_counts.sort_index().plot(kind='bar', color='steelblue', edgecolor='black')\n",
        "plt.title(\"Number of Articles Per Cluster\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Number of Articles\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9WkvWEVo8iVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploration of Test Dataset"
      ],
      "metadata": {
        "id": "QLD9cgtf3Rvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "doc_df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/My Folder/topic_model_test_new.csv')\n",
        "\n",
        "doc_df_test.head()\n",
        "\n",
        "# Sum up the number of articles in each subject\n",
        "subject_counts = doc_df_test.iloc[:, 3:].sum()  # Summing only subject columns\n",
        "\n",
        "# Print subject counts\n",
        "print(subject_counts)\n",
        "\n",
        "# Count articles with multiple subjects\n",
        "doc_df_test['num_subjects'] = doc_df_test.iloc[:, 3:].sum(axis=1)  # Sum across rows\n",
        "multi_topic_count = (doc_df_test['num_subjects'] > 1).sum()  # Count articles in multiple subjects\n",
        "\n",
        "# Add multi-topic count to the subject count dictionary\n",
        "subject_counts[\"Multiple Subjects\"] = multi_topic_count\n",
        "# Count articles with single and multiple topics\n",
        "single_topic_count = (doc_df_test['num_subjects'] == 1).sum()\n",
        "multi_topic_count = (doc_df_test['num_subjects'] > 1).sum()\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "topic_distribution = pd.DataFrame({\n",
        "    'Count': [single_topic_count, multi_topic_count]\n",
        "}, index=['Single Subject', 'Multiple Subjects'])\n",
        "\n",
        "# Plot\n",
        "topic_distribution.plot(kind='bar', color=['lightcoral', 'steelblue'], edgecolor='black', figsize=(8,5))\n",
        "plt.title(\"Distribution of Articles by Single vs. Multiple Subject Assignment\")\n",
        "plt.xlabel(\"Category\")\n",
        "plt.ylabel(\"Number of Articles\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "subject_counts.sort_values().plot(kind='bar', color='skyblue', edgecolor='black')\n",
        "\n",
        "plt.title(\"Number of Articles in Each Subject (Including Multi-Topic Articles)\")\n",
        "plt.xlabel(\"Subjects\")\n",
        "plt.ylabel(\"Number of Articles\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "# Selecting only subject columns for correlation analysis\n",
        "subject_columns = doc_df_test.iloc[:, 3:-1]  # Excludes num_subjects column\n",
        "correlation_matrix = subject_columns.corr()\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = subject_columns.corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "plt.title(\"Correlation Between Subject Tags\")\n",
        "plt.xlabel(\"Subjects\")\n",
        "plt.ylabel(\"Subjects\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hkolsl1pWbbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing LDA Model"
      ],
      "metadata": {
        "id": "uiNB2nfgpC4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_vectorized_test = my_vectorizer.transform(doc_df_test['ABSTRACT'])\n",
        "\n",
        "\n",
        "lda_top_test = lda_model_best.transform(abstract_vectorized_test)\n",
        "lda_top_test.shape\n",
        "\n",
        "topic_df_test = pd.DataFrame(lda_top_test)\n",
        "topic_df_test.columns = ['topic_' + str(i) for i in range(topic_df_test.shape[1])]\n",
        "topic_df_test.reset_index(drop=True)\n",
        "\n",
        "topic_df_test['topic_with_highest_score'] = topic_df_test.idxmax(axis=1)\n",
        "\n",
        "doc_df_test_with_topics = pd.concat([doc_df_test, topic_df_test], axis=1)\n",
        "\n",
        "doc_df_test_with_topics.info()\n",
        "doc_df_test_with_topics.head()"
      ],
      "metadata": {
        "id": "jBu14IFCpDR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_labels = ['Computer Science',\t'Physics', 'Mathematics',\t'Statistics',\t'Quantitative Biology','Quantitative Finance']\n",
        "\n",
        "for i in range(best_num_topics):\n",
        "  topic_lda= 'topic_' + str(i)\n",
        "  for topic_label in topic_labels:\n",
        "    temp_df = doc_df_test_with_topics[doc_df_test_with_topics['topic_with_highest_score'] == topic_lda]\n",
        "    print(topic_lda, '->', topic_label, temp_df[topic_label].sum())\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "invHzG_4DarA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store the assigned labels\n",
        "topic_to_label = {}\n",
        "\n",
        "# Loop through each LDA topic and assign the most frequent real label\n",
        "for i in range(best_num_topics):\n",
        "    topic_lda = 'topic_' + str(i)\n",
        "\n",
        "    # Subset only documents assigned to this topic\n",
        "    temp_df = doc_df_test_with_topics[doc_df_test_with_topics['topic_with_highest_score'] == topic_lda]\n",
        "\n",
        "    # Find the most common real label in this topic\n",
        "    most_common_label = temp_df[topic_labels].sum().idxmax()\n",
        "\n",
        "    # Store in the dictionary\n",
        "    topic_to_label[topic_lda] = most_common_label\n",
        "\n",
        "# Display assigned topic labels\n",
        "print(\"Assigned Topic Labels:\", topic_to_label)"
      ],
      "metadata": {
        "id": "fn-CiSia120T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column to store the predicted subject based on LDA topics\n",
        "doc_df_test_with_topics['Predicted_Subject'] = doc_df_test_with_topics['topic_with_highest_score'].map(topic_to_label)\n",
        "\n",
        "# Show some results\n",
        "print(doc_df_test_with_topics[['TITLE', 'topic_with_highest_score', 'Predicted_Subject']].head())"
      ],
      "metadata": {
        "id": "uJtf-d031-gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Get true labels (real subject per document)\n",
        "true_labels = doc_df_test_with_topics[topic_labels].idxmax(axis=1)\n",
        "\n",
        "# Get predicted labels (LDA assigned subjects)\n",
        "predicted_labels = doc_df_test_with_topics['Predicted_Subject']\n",
        "\n",
        "# Compute classification report\n",
        "print(classification_report(true_labels, predicted_labels))"
      ],
      "metadata": {
        "id": "siIejgwI2G5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Doc2Vec Model"
      ],
      "metadata": {
        "id": "s1AjWhMjo62g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "test_docs = doc_df_test['ABSTRACT'].tolist()\n",
        "\n",
        "test_vectors = np.array([doc_model.infer_vector(my_preprocessing(doc).split()) for doc in test_docs])\n",
        "\n",
        "closest_clusters = np.argmin(cdist(test_vectors, kmeans.cluster_centers_), axis=1)\n",
        "\n",
        "# Store test cluster assignments\n",
        "doc_df_test['Predicted_Cluster'] = closest_clusters\n",
        "\n",
        "# Show results\n",
        "print(doc_df_test[['TITLE', 'Predicted_Cluster']].head())\n"
      ],
      "metadata": {
        "id": "R2EK7H6TSe1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_topic_counts = {}\n",
        "\n",
        "# Loop through each cluster\n",
        "for cluster in np.unique(closest_clusters):\n",
        "    print(f\"\\nCluster {cluster}\")  # Print cluster header\n",
        "\n",
        "    # Subset only documents assigned to this cluster\n",
        "    temp_df = doc_df_test[doc_df_test['Predicted_Cluster'] == cluster]\n",
        "\n",
        "    # Loop through all subject labels and print counts\n",
        "    for topic_label in topic_columns:\n",
        "        topic_count = temp_df[topic_label].sum()\n",
        "        print(f\"Cluster {cluster} -> {topic_label}: {topic_count}\")"
      ],
      "metadata": {
        "id": "TMQitKgv4fNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_columns = ['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']\n",
        "\n",
        "# Find the majority label for each cluster\n",
        "cluster_topic_mapping = {}\n",
        "\n",
        "for cluster in np.unique(closest_clusters):\n",
        "    cluster_indices = doc_df_test['Predicted_Cluster'] == cluster\n",
        "    majority_topic = doc_df_test.loc[cluster_indices, topic_columns].sum().idxmax()\n",
        "    cluster_topic_mapping[cluster] = majority_topic\n",
        "\n",
        "# Assign mapped topics\n",
        "doc_df_test['Mapped_Topic'] = doc_df_test['Predicted_Cluster'].map(cluster_topic_mapping)\n",
        "\n",
        "# Show some results\n",
        "print(doc_df_test[['TITLE', 'Predicted_Cluster', 'Mapped_Topic']].head())"
      ],
      "metadata": {
        "id": "wga35TaA2isl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get true labels\n",
        "true_labels = doc_df_test[topic_columns].idxmax(axis=1)  # Assigns each article its actual topic\n",
        "predicted_labels = doc_df_test['Mapped_Topic']\n",
        "\n",
        "# Compute classification report\n",
        "print(classification_report(true_labels, predicted_labels))"
      ],
      "metadata": {
        "id": "fcwunCoy2o24"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}