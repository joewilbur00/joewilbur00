{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Qmf_XID3mmm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ceaning Dataset"
      ],
      "metadata": {
        "id": "He-IW-rp6swI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "3ZsNaEyK4V4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/Final_Project/ACL19_Release'"
      ],
      "metadata": {
        "id": "KQTWCirL4ZWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Audio Files from earnings call folders\n",
        "\n",
        "for company_folder in os.listdir(base_path):\n",
        "    full_path = os.path.join(base_path, company_folder)\n",
        "    ceo_audio_folder = os.path.join(full_path, \"CEO\")\n",
        "\n",
        "    if os.path.isdir(ceo_audio_folder):\n",
        "        for file in os.listdir(ceo_audio_folder):\n",
        "            if file.endswith(\".mp3\"):\n",
        "                try:\n",
        "                    os.remove(os.path.join(ceo_audio_folder, file))\n",
        "                    print(f\"Deleted: {file}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error deleting {file}: {e}\")"
      ],
      "metadata": {
        "id": "3D0A3xBm4ilX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Separate company from date\n",
        "\n",
        "rows = []\n",
        "\n",
        "for folder in os.listdir(base_path):\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "\n",
        "    if os.path.isdir(folder_path):\n",
        "        try:\n",
        "            # Split into company and date\n",
        "            company_name, date_str = folder.rsplit('_', 1)\n",
        "            earnings_date = datetime.strptime(date_str, '%Y%m%d').date()\n",
        "\n",
        "            # Read TextSequence.txt\n",
        "            text_file = os.path.join(folder_path, \"TextSequence.txt\")\n",
        "            if os.path.isfile(text_file):\n",
        "                with open(text_file, 'r', encoding='utf-8') as f:\n",
        "                    text = f.read().strip()\n",
        "                    rows.append({\n",
        "                        'Company_Name': company_name,\n",
        "                        'Earnings_Call_Date': earnings_date,\n",
        "                        'Text': text\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {folder}: {e}\")\n",
        "\n",
        "# Save results to CSV\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(\"earnings_calls_cleaned.csv\", index=False)"
      ],
      "metadata": {
        "id": "pLUoP0U059Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/Final_Project/earnings_calls_cleaned.csv\", index=False)"
      ],
      "metadata": {
        "id": "DFaedSMf8ycx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earnings_call_dataset = pd.read_csv('/content/drive/MyDrive/Final_Project/earnings_calls_cleaned.csv')"
      ],
      "metadata": {
        "id": "gnBlj9Yb8QUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earnings_call_dataset.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3fBr7Eti8VOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earnings_call_dataset.info()"
      ],
      "metadata": {
        "id": "70Lqz1EV8quY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Company Codes and Industries to Dataset"
      ],
      "metadata": {
        "id": "bZIE4B5t_hEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy[speedup] pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1zRiJI5D-S0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "fuzz.ratio('A.O. Smith Corp', 'A.O. Smith Corporation Common Stock')"
      ],
      "metadata": {
        "id": "C_d1NoFPJEJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nasdaq_df = pd.read_csv('/content/drive/MyDrive/Final_Project/nasdaq_screener_1746290920721.csv')\n",
        "\n",
        "# Clean the Name column\n",
        "nasdaq_df['Name'] = nasdaq_df['Name'].str.replace('Common Stock', '', regex=False).str.strip()\n",
        "\n",
        "nasdaq_df = nasdaq_df.drop(columns=['Last Sale', 'Net Change', '% Change', 'Market Cap', 'IPO Year', 'Volume'])\n",
        "\n",
        "nasdaq_df.head(10)"
      ],
      "metadata": {
        "id": "lhJm2sSTRqq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nasdaq_df.to_csv(\"/content/drive/MyDrive/Final_Project/nasdaq.csv\", index=False)"
      ],
      "metadata": {
        "id": "dvKDP1SQSvFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earnings_df = pd.read_csv('/content/drive/MyDrive/Final_Project/earnings_calls_cleaned.csv')\n",
        "reference_df = pd.read_csv('/content/drive/MyDrive/Final_Project/nasdaq.csv')"
      ],
      "metadata": {
        "id": "wYBv0TpJYepa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import process\n",
        "\n",
        "# Clean company names\n",
        "earnings_df['Company_Name_cleaned'] = earnings_df['Company_Name'].str.lower().str.strip()\n",
        "reference_df['Name_cleaned'] = reference_df['Name'].str.lower().str.strip()\n",
        "\n",
        "# Reference list of cleaned names\n",
        "reference_names = reference_df['Name_cleaned'].tolist()\n",
        "\n",
        "# Create match columns\n",
        "matched_names = []\n",
        "match_scores = []\n",
        "\n",
        "# Fuzzy match each company in earnings_df\n",
        "for name in earnings_df['Company_Name_cleaned']:\n",
        "    match, score = process.extractOne(name, reference_names)\n",
        "    if score >= 85:  # Strict threshold to prevent false positives\n",
        "        matched_names.append(match)\n",
        "        match_scores.append(score)\n",
        "    else:\n",
        "        matched_names.append(None)\n",
        "        match_scores.append(None)\n",
        "\n",
        "# Add match info to earnings_df\n",
        "earnings_df['Matched_Name'] = matched_names\n",
        "earnings_df['Match_Score'] = match_scores\n",
        "\n",
        "# Merge only on high-confidence matches\n",
        "merged_df = earnings_df.merge(\n",
        "    reference_df,\n",
        "    left_on='Matched_Name',\n",
        "    right_on='Name_cleaned',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Final selection\n",
        "final_df = merged_df[[\n",
        "    'Company_Name', 'Earnings_Call_Date', 'Text',\n",
        "    'Symbol', 'Country', 'Sector', 'Industry', 'Match_Score'\n",
        "]].rename(columns={'Symbol': 'Stock_Ticker'})\n",
        "\n",
        "\n",
        "final_df.to_csv('/content/drive/MyDrive/Final_Project/enriched_earnings_calls.csv', index=False)"
      ],
      "metadata": {
        "id": "ugo_j_M3B0LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KHDgTYROV9D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not receiving great results with fuzzy, attemptin TF-IDF vector similarities"
      ],
      "metadata": {
        "id": "PhexSjPnXVQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean company name columns\n",
        "import re\n",
        "\n",
        "def clean_name(name):\n",
        "    name = name.lower()\n",
        "    name = re.sub(r'[^a-z0-9 ]', '', name)         # remove punctuation\n",
        "    name = re.sub(r'\\b(inc|co|corp|ltd|plc|llc)\\b', '', name)  # remove suffixes\n",
        "    return name.strip()\n",
        "\n",
        "earnings_df['Company_Name_cleaned'] = earnings_df['Company_Name'].apply(clean_name)\n",
        "reference_df['Name_cleaned'] = reference_df['Name'].apply(clean_name)"
      ],
      "metadata": {
        "id": "dwad1MsrXayJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Fit TF-IDF on both sets of names\n",
        "vectorizer = TfidfVectorizer().fit(earnings_df['Company_Name_cleaned'].tolist() + reference_df['Name_cleaned'].tolist())\n",
        "\n",
        "earnings_vecs = vectorizer.transform(earnings_df['Company_Name_cleaned'])\n",
        "reference_vecs = vectorizer.transform(reference_df['Name_cleaned'])\n",
        "\n",
        "# Compute pairwise cosine similarity\n",
        "similarity_matrix = cosine_similarity(earnings_vecs, reference_vecs)\n",
        "best_match_idx = np.argmax(similarity_matrix, axis=1)\n",
        "best_scores = similarity_matrix[np.arange(len(earnings_df)), best_match_idx]\n",
        "\n",
        "# Match only if similarity score is high enough\n",
        "matched_names = []\n",
        "for i, score in enumerate(best_scores):\n",
        "    if score > 0.85:  # tighten this threshold\n",
        "        matched_names.append(reference_df.iloc[best_match_idx[i]]['Name_cleaned'])\n",
        "    else:\n",
        "        matched_names.append(None)\n",
        "\n",
        "earnings_df['Matched_Name'] = matched_names"
      ],
      "metadata": {
        "id": "hdtNQLkcXkTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = earnings_df.merge(reference_df, left_on='Matched_Name', right_on='Name_cleaned', how='left')\n",
        "\n",
        "merged_df.info()"
      ],
      "metadata": {
        "id": "19lkSXCqYr4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_view = merged_df.drop(columns=['Text'])\n",
        "\n",
        "df_view"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zxBLEsq6ZfEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.drop(columns=['Company_Name_cleaned', 'Matched_Name', 'Name', 'Name_cleaned'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9FzXVpo6b9O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.rename(columns={'Symbol': 'Stock_Ticker'})"
      ],
      "metadata": {
        "id": "7qVEIQbvcZHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.dropna()\n",
        "\n",
        "merged_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dj1LALXqc0ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.to_csv('/content/drive/MyDrive/Final_Project/enriched_earnings_calls.csv', index=False)"
      ],
      "metadata": {
        "id": "czTVR0PSjhuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_count = merged_df['Company_Name'].nunique()\n",
        "print(f\"Number of unique companies: {unique_count}\")"
      ],
      "metadata": {
        "id": "7n3jLi3Jj-BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_counts = merged_df['Company_Name'].value_counts()\n",
        "print(company_counts)"
      ],
      "metadata": {
        "id": "Y3w6oo9-kp2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_counts_df = company_counts.reset_index()\n",
        "company_counts_df.columns = ['Company_Name', 'Row_Count']\n",
        "company_counts_df"
      ],
      "metadata": {
        "id": "_-pHCk2XkzxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_companies = company_counts[company_counts >= 2].index\n",
        "\n",
        "filtered_df = merged_df[merged_df['Company_Name'].isin(valid_companies)]\n",
        "\n",
        "filtered_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2DiYCSLhlZD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count unique companies by sector\n",
        "companies_by_sector = merged_df.groupby('Sector')['Company_Name'].nunique().sort_values(ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "companies_by_sector.plot(kind='bar')\n",
        "plt.title('Number of Unique Companies by Sector')\n",
        "plt.xlabel('Sector')\n",
        "plt.ylabel('Unique Company Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RYTp2cF5pRzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retreive Stock Data"
      ],
      "metadata": {
        "id": "s6X0Cpgz8iwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "!pip install yfinance tqdm"
      ],
      "metadata": {
        "id": "iHkwkb0J8lVX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.read_csv('/content/drive/MyDrive/Final_Project/enriched_earnings_calls.csv')\n",
        "\n",
        "merged_df"
      ],
      "metadata": {
        "id": "OIoUYBzuYDC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "ticker = yf.Ticker(\"ABBV\")\n",
        "hist = ticker.history(start=\"2017-07-20\", end=\"2017-08-01\")\n",
        "print(hist)"
      ],
      "metadata": {
        "id": "e0aPvVYrw-Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "records = []\n",
        "\n",
        "for idx, row in merged_df.iterrows():\n",
        "    ticker = row['Stock_Ticker']\n",
        "    call_date = pd.to_datetime(row['Earnings_Call_Date'])\n",
        "\n",
        "    if pd.notnull(ticker) and pd.notnull(call_date):\n",
        "        try:\n",
        "            start = call_date - timedelta(days=5)\n",
        "            end = call_date + timedelta(days=5)\n",
        "            data = yf.download(ticker, start=start, end=end, progress=False)\n",
        "\n",
        "            if not data.empty:\n",
        "                for date, row_data in data.iterrows():\n",
        "                    records.append({\n",
        "                        'Ticker': ticker,\n",
        "                        'Call_Date': call_date.date(),\n",
        "                        'Price_Date': date.date(),\n",
        "                        'Close': row_data['Close']\n",
        "                    })\n",
        "            time.sleep(1)\n",
        "        except Exception as e:\n",
        "            print(f\"[{ticker} on {call_date}] Failed: {e}\")"
      ],
      "metadata": {
        "id": "e8MGDvNm8pDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_df = pd.DataFrame(records)"
      ],
      "metadata": {
        "id": "_1gjEqID0kJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def use_regex(input_text):\n",
        "    input_text = str(input_text)  # ensure it's a string\n",
        "    pattern = re.compile(r\"[0-9]*\\.[0-9]+\")\n",
        "    match = pattern.search(input_text)\n",
        "    if match:\n",
        "        return float(match.group(0))\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "EvnP47XI0o15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_df['Close_Clean'] = price_df['Close'].apply(use_regex)"
      ],
      "metadata": {
        "id": "gsM5JtI35D6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_df.drop(columns=['Close'], inplace=True)"
      ],
      "metadata": {
        "id": "HIT7xa3M5YND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_df.to_csv('/content/drive/MyDrive/Final_Project/stock_price_data.csv', index=False)"
      ],
      "metadata": {
        "id": "vYZW8qnyCZMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_df = pd.read_csv('/content/drive/MyDrive/Final_Project/stock_price_data.csv')"
      ],
      "metadata": {
        "id": "T4wCzSF9LaeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_df.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wKWqhfT2C1oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert dates\n",
        "price_df['Call_Date'] = pd.to_datetime(price_df['Call_Date'])\n",
        "price_df['Price_Date'] = pd.to_datetime(price_df['Price_Date'])\n",
        "\n",
        "metrics = []\n",
        "\n",
        "# Group by ticker and call date\n",
        "grouped = price_df.groupby(['Ticker', 'Call_Date'])\n",
        "\n",
        "for (ticker, call_date), group in grouped:\n",
        "    group = group.sort_values('Price_Date')\n",
        "\n",
        "    # Pre- and post-call windows\n",
        "    pre_call = group[group['Price_Date'] < call_date]\n",
        "    post_call_inclusive = group[group['Price_Date'] >= call_date]\n",
        "\n",
        "    price_change_1d = None\n",
        "    volatility_3d = None\n",
        "    avg_change = None\n",
        "\n",
        "    # 1-day price change\n",
        "    if not pre_call.empty and len(post_call_inclusive) >= 2:\n",
        "        try:\n",
        "            pre_close = pre_call.iloc[-1]['Close_Clean']\n",
        "            post_close = post_call_inclusive.iloc[1]['Close_Clean']\n",
        "            price_change_1d = (post_close - pre_close) / pre_close\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Volatility (first 3 trading days including call)\n",
        "    try:\n",
        "        post_returns = post_call_inclusive['Close_Clean'].pct_change().dropna()\n",
        "        volatility_3d = post_returns.head(3).std()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Average price change (3 pre vs 3 post incl. call)\n",
        "    try:\n",
        "        avg_pre_prices = pre_call['Close_Clean'].tail(3)\n",
        "        avg_post_prices = post_call_inclusive['Close_Clean'].head(3)\n",
        "\n",
        "        if len(avg_pre_prices) >= 2 and len(avg_post_prices) >= 2:\n",
        "            avg_pre = avg_pre_prices.mean()\n",
        "            avg_post = avg_post_prices.mean()\n",
        "            avg_change = (avg_post - avg_pre) / avg_pre\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    metrics.append({\n",
        "        'Stock_Ticker': ticker,\n",
        "        'Earnings_Call_Date': call_date,\n",
        "        'Price_Change_1d': price_change_1d,\n",
        "        'Volatility_2d': volatility_3d,\n",
        "        'Avg_Price_Change': avg_change\n",
        "    })\n",
        "\n",
        "price_metrics_df = pd.DataFrame(metrics)"
      ],
      "metadata": {
        "id": "oTRkkAGIE7nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_metrics_df.info()"
      ],
      "metadata": {
        "id": "dk6LuxQ4Fcdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Earnings Calls"
      ],
      "metadata": {
        "id": "2nfTJjAHgZLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, names\n",
        "import string\n",
        "\n",
        "# One-time downloads\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('names')\n",
        "\n",
        "# Initialize components\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#Remove names\n",
        "name_list = set([name.lower() for name in names.words()])\n",
        "\n",
        "# Add custom stopwords\n",
        "custom_stopwords = set(['quarter', 'guidance', 'call', 'fiscal', 'update', 'thank', 'thanks', 'questions', 'indiscernible', 'inaudible', 'okay', 'today', 'year', 'wa', 'think', 'million', 'billion', 'one', 'two', 'three', 'well', 'would', 'weve', 'going', 'really'])\n",
        "stop_words.update(custom_stopwords)\n",
        "\n",
        "# Acronym mapping\n",
        "acronym_map = {\n",
        "    'eps': 'earnings_per_share',\n",
        "    'gaap': 'accounting_standard',\n",
        "    'ebit': 'operating_income'\n",
        "}\n",
        "\n",
        "def my_preprocessor(text):\n",
        "    # Lowercase and remove known tags\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?technical difficulty.*?\\]', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove numbers, punctuation, and special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Replace acronyms\n",
        "    tokens = [acronym_map.get(token, token) for token in tokens]\n",
        "\n",
        "    # Remove short workds\n",
        "    tokens = [token for token in tokens if len(token) > 2]\n",
        "\n",
        "    # Lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Remove common names\n",
        "    tokens = [token for token in tokens if token not in name_list]\n",
        "\n",
        "    # Return clean string\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "3uMMafFggb_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.to_csv('/content/drive/MyDrive/Final_Project/enriched_earnings_calls_processed.csv', index=False)"
      ],
      "metadata": {
        "id": "QoDR0H8qqz2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.read_csv('/content/drive/MyDrive/Final_Project/enriched_earnings_calls_processed.csv')"
      ],
      "metadata": {
        "id": "zGQC-mpaKofV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "ET6fbPvaLoXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic Modelling"
      ],
      "metadata": {
        "id": "rlggBvMILNO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempting LDA First"
      ],
      "metadata": {
        "id": "iq6WoD6NW4li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y smart_open\n",
        "!pip install --upgrade smart_open\n",
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "etnADvW8MaOO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim\n",
        "!pip install smart_open==5.2.1"
      ],
      "metadata": {
        "id": "-NQoNKP5Nvec",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(preprocessor=my_preprocessor, max_features=5000)\n",
        "count_matrix = vectorizer.fit_transform(merged_df['Text'])\n",
        "\n",
        "# Step 2: Try different topic counts\n",
        "topic_models = {}\n",
        "for num_topics in [3, 4, 5, 6]:\n",
        "    print(f\"Training LDA with {num_topics} topics...\")\n",
        "\n",
        "    lda_model = LatentDirichletAllocation(\n",
        "        n_components=num_topics,\n",
        "        learning_method='online',\n",
        "        random_state=42,\n",
        "        max_iter=10\n",
        "    )\n",
        "    lda_model.fit(count_matrix)\n",
        "\n",
        "    # Store the model\n",
        "    topic_models[num_topics] = lda_model\n",
        "\n",
        "    # Evaluate using log perplexity\n",
        "    perplexity = lda_model.perplexity(count_matrix)\n",
        "    print(f\"Perplexity for {num_topics} topics: {perplexity:.2f}\")\n",
        "\n",
        "    # Print top words for each topic\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    for idx, topic in enumerate(lda_model.components_):\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
        "        print(f\"Topic {idx}: {', '.join(top_words)}\")\n",
        "    print(\"\\n\" + \"-\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "dZ839qIhJPli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_num_topics = 4  # Higher perplexity score than 3, but slightly more diversity in topics\n",
        "\n",
        "lda_model = LatentDirichletAllocation(n_components=best_num_topics,\n",
        "                                           learning_method='online',\n",
        "                                           random_state=42,\n",
        "                                           max_iter=10)\n",
        "\n",
        "# Fit the model with count matrix\n",
        "lda_best = lda_model.fit_transform(count_matrix)"
      ],
      "metadata": {
        "id": "pDnxnnVuSShX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis\n",
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "id": "URioeoaWUT7Q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model as sklearnvis\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis_data = sklearnvis.prepare(lda_model, count_matrix, vectorizer)\n",
        "pyLDAvis.display(vis_data)"
      ],
      "metadata": {
        "id": "wlyMZkZDULCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now seeing if SBERT improves results"
      ],
      "metadata": {
        "id": "b6_JRsqIW74w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "\n",
        "# Reinstall numpy first\n",
        "!pip install --force-reinstall numpy==1.24.4\n",
        "\n",
        "# Then reinstall scipy to match numpy\n",
        "!pip install --force-reinstall scipy==1.10.1\n",
        "\n",
        "# Then reinstall BERTopic cleanly\n",
        "!pip install --force-reinstall bertopic\n",
        "\n",
        "# Also reinstall sentence-transformers just in case\n",
        "!pip install --force-reinstall sentence-transformers"
      ],
      "metadata": {
        "id": "v7uY38F1Y7tl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 --upgrade --force-reinstall"
      ],
      "metadata": {
        "id": "2dM67ON_NEwO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "\n",
        "docs = merged_df['Text']  # Avoid issues with NaNs\n",
        "\n",
        "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Compute embeddings (with progress bar)\n",
        "sbert_embeddings = sbert_model.encode(docs, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "lqCSlgYsXRHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as sc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sc.dendrogram(sc.linkage(sbert_embeddings, method='ward'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JV0WosIIac_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "for k in range(3, 7):  # Try different cluster numbers\n",
        "    my_clustering = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
        "    SBERT_cluster_labels = my_clustering.fit_predict(sbert_embeddings)\n",
        "    score = silhouette_score(sbert_embeddings, SBERT_cluster_labels)\n",
        "    print(f\"Clusters: {k}, Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "JzSEvn8xasb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "for k in range(3, 7):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    SBERT_cluster_labels_kmeans = kmeans.fit_predict(sbert_embeddings)\n",
        "    score = silhouette_score(sbert_embeddings, SBERT_cluster_labels_kmeans)\n",
        "    print(f\"K-Means Clusters: {k}, Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "e9mjqZpUa_bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "USE Model"
      ],
      "metadata": {
        "id": "vB2FgVVebUWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "use_embeddings = use_model(docs)\n",
        "\n",
        "sc.dendrogram(sc.linkage(use_embeddings, method='ward'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R3b5N-BcbTpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(3, 7):  # Try different cluster numbers\n",
        "    my_clustering = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
        "    USE_cluster_labels = my_clustering.fit_predict(use_embeddings)\n",
        "    score = silhouette_score(use_embeddings, USE_cluster_labels)\n",
        "    print(f\"Clusters: {k}, Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "Mp3vb4Q8bYwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(3, 7):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    USE_cluster_labels_kmeans = kmeans.fit_predict(use_embeddings)\n",
        "    score = silhouette_score(use_embeddings, USE_cluster_labels_kmeans)\n",
        "    print(f\"K-Means Clusters: {k}, Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "Kpn6iwvsbhnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmean_best = KMeans(n_clusters=5, random_state=42)\n",
        "best_cluster_labels = kmeans.fit_predict(use_embeddings)"
      ],
      "metadata": {
        "id": "Kk32rCxghB6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding USE and LDA Topics to merged_df"
      ],
      "metadata": {
        "id": "3_hQggRzf6Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['USE_Topic'] = best_cluster_labels\n",
        "\n",
        "for i in range(lda_best.shape[1]):\n",
        "    merged_df[f'LDA_Topic_{i}'] = lda_best[:, i]\n",
        "\n",
        "merged_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C_1eLSWUf9mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Analysis"
      ],
      "metadata": {
        "id": "fXG55FOukMvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install -U sentencepiece"
      ],
      "metadata": {
        "id": "6L1EKfOxkOaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "import torch\n",
        "\n",
        "# Load FinBERT (Prosus version fine-tuned on financial text)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
      ],
      "metadata": {
        "id": "NE4khWNwkS73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_finbert_sentiment(text):\n",
        "    try:\n",
        "        inputs = tokenizer(text[:512], return_tensors=\"pt\", truncation=True)\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "        scores = softmax(logits.numpy()[0])\n",
        "        return {\n",
        "            'FinBERT_Positive': scores[0],\n",
        "            'FinBERT_Neutral': scores[1],\n",
        "            'FinBERT_Negative': scores[2],\n",
        "            'FinBERT_Polarity': scores[0] - scores[2]  # Positive - Negative\n",
        "        }\n",
        "    except:\n",
        "        return {\n",
        "            'FinBERT_Positive': None,\n",
        "            'FinBERT_Neutral': None,\n",
        "            'FinBERT_Negative': None,\n",
        "            'FinBERT_Polarity': None\n",
        "        }"
      ],
      "metadata": {
        "id": "0slqg_-fkW8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_scores = merged_df['Text'].apply(get_finbert_sentiment)\n",
        "sentiment_df = pd.DataFrame(sentiment_scores.tolist())\n",
        "\n",
        "# Merge back into merged_df\n",
        "merged_df = pd.concat([merged_df, sentiment_df], axis=1)"
      ],
      "metadata": {
        "id": "hRu1HyvfkiV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge all data together now"
      ],
      "metadata": {
        "id": "C8zkxGVlIC-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure date format matches\n",
        "merged_df['Earnings_Call_Date'] = pd.to_datetime(merged_df['Earnings_Call_Date'])\n",
        "\n",
        "# Merge on Stock_Ticker and Earnings_Call_Date\n",
        "merged_df = merged_df.merge(\n",
        "    price_metrics_df,\n",
        "    how='left',\n",
        "    on=['Stock_Ticker', 'Earnings_Call_Date']\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pCk-OlrtICMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.info()"
      ],
      "metadata": {
        "id": "h87vmfa7IxpA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.to_csv('/content/drive/MyDrive/Final_Project/final_earnings_calls.csv', index=False)"
      ],
      "metadata": {
        "id": "8Xu5gYsQJSUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seeing how the USE Model predicts topics"
      ],
      "metadata": {
        "id": "nRnB_FA9khMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "industry_labels = pd.get_dummies(merged_df['Sector'])\n",
        "merged_with_labels = pd.concat([merged_df, industry_labels], axis=1)"
      ],
      "metadata": {
        "id": "kQxO1DCMkkN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_to_label = {}\n",
        "\n",
        "# Loop through each USE topic and assign the most frequent industry\n",
        "for topic in merged_with_labels['USE_Topic'].unique():\n",
        "    temp_df = merged_with_labels[merged_with_labels['USE_Topic'] == topic]\n",
        "    most_common_label = temp_df[industry_labels.columns].sum().idxmax()\n",
        "    topic_to_label[topic] = most_common_label\n",
        "\n",
        "print(\"Assigned Topics to Industry:\", topic_to_label)"
      ],
      "metadata": {
        "id": "yMFlwagmkln3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_with_labels['Predicted_Industry'] = merged_with_labels['USE_Topic'].map(topic_to_label)"
      ],
      "metadata": {
        "id": "YLXh6XPXk-XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "true_labels = merged_with_labels[industry_labels.columns].idxmax(axis=1)\n",
        "predicted_labels = merged_with_labels['Predicted_Industry']\n",
        "\n",
        "print(classification_report(true_labels, predicted_labels))"
      ],
      "metadata": {
        "id": "lw3OJ3dslAcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis"
      ],
      "metadata": {
        "id": "KyhP2oRAJR0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.read_csv('/content/drive/MyDrive/Final_Project/final_earnings_calls.csv')"
      ],
      "metadata": {
        "id": "b0TtvkXRdaEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df[['FinBERT_Polarity', 'Price_Change_1d', 'Volatility_2d', 'Avg_Price_Change']].corr()"
      ],
      "metadata": {
        "id": "9zrrQBIZeM91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test on Price Volatility"
      ],
      "metadata": {
        "id": "ORkLSS4bq1Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "merged_df['High_Volatility'] = merged_df['Volatility_2d'] > merged_df['Volatility_2d'].median()\n",
        "\n",
        "use_topic_dummies = pd.get_dummies(merged_df['USE_Topic'], prefix='Topic')\n",
        "\n",
        "\n",
        "X = pd.concat([\n",
        "    merged_df[['FinBERT_Positive', 'FinBERT_Neutral', 'FinBERT_Negative']],\n",
        "    use_topic_dummies\n",
        "], axis=1)\n",
        "y = merged_df['High_Volatility'].astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "k237Ner2epDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.bar(range(len(feature_names)), importances[indices])\n",
        "plt.xticks(range(len(feature_names)), feature_names[indices], rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9SahsUgCplr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test on AVG Price Change"
      ],
      "metadata": {
        "id": "nJ1SBthXmP0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Features\n",
        "X = pd.concat([\n",
        "    merged_df[['FinBERT_Positive', 'FinBERT_Neutral', 'FinBERT_Negative']],\n",
        "    pd.get_dummies(merged_df['USE_Topic'], prefix='Topic')\n",
        "], axis=1)\n",
        "\n",
        "# Target\n",
        "y = merged_df['Avg_Price_Change']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "reg = RandomForestRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = reg.predict(X_test)\n",
        "print(\"R²:\", r2_score(y_test, y_pred))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))"
      ],
      "metadata": {
        "id": "EE7hJhoemS_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train, y_train)\n",
        "\n",
        "print(\"R²:\", linreg.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "qtuzVqVJopo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test on if price moves up or down"
      ],
      "metadata": {
        "id": "r3HiNTttq5bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['Price_Up'] = merged_df['Price_Change_1d'].apply(\n",
        "    lambda x: 1 if x > 0.01 else 0 if x < -0.01 else np.nan\n",
        ")\n",
        "merged_df = merged_df.dropna(subset=['Price_Up'])"
      ],
      "metadata": {
        "id": "9zJgVxSgqSVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_topic_dummies = pd.get_dummies(merged_df['USE_Topic'], prefix='Topic')\n",
        "X = pd.concat([\n",
        "    merged_df[['FinBERT_Positive', 'FinBERT_Neutral', 'FinBERT_Negative']],\n",
        "    use_topic_dummies\n",
        "], axis=1)\n",
        "\n",
        "y = merged_df['Price_Up'].astype(int)"
      ],
      "metadata": {
        "id": "ipVOV85YqUfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "PlSVsdzNqZf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = clf.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.bar(range(len(feature_names)), importances[indices])\n",
        "plt.xticks(range(len(feature_names)), feature_names[indices], rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MCI3u4paqexb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test using only sentiment scores"
      ],
      "metadata": {
        "id": "tVDecRnkrElY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (sentiment only)\n",
        "X = merged_df[['FinBERT_Positive', 'FinBERT_Neutral', 'FinBERT_Negative']]\n",
        "\n",
        "# Define target (adjust for your use case — either Price_Up or High_Volatility)\n",
        "y = merged_df['Price_Up'].astype(int)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit Random Forest\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "qXh3jWK8rDwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = clf.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.bar(range(len(feature_names)), importances[indices])\n",
        "plt.xticks(range(len(feature_names)), feature_names[indices], rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B1FqORq7t-Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (sentiment only)\n",
        "X = merged_df[['FinBERT_Positive', 'FinBERT_Neutral', 'FinBERT_Negative']]\n",
        "\n",
        "# Define target (adjust for your use case — either Price_Up or High_Volatility)\n",
        "y = merged_df['High_Volatility'].astype(int)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit Random Forest\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "DKVK4hHuuaGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.head()"
      ],
      "metadata": {
        "id": "zzGNLup1rXte"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}