{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF with Logistic Regression"
      ],
      "metadata": {
        "id": "AvYKwC5Tn5U2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAqjUQIfN0-Z",
        "outputId": "8c090f7e-5cb8-4fe1-e81f-e59c24527eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Logistic Regression Accuracy: 0.75895\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "import string\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
        "from tkinter.constants import X\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "tw_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/My Folder/tweeter_training (3).csv', header=None, encoding=\"ISO-8859-1\")\n",
        "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
        "tw_df.columns = column_names\n",
        "tweet_column_index = 5\n",
        "\n",
        "\n",
        "# Preprocessing function with lemmatization\n",
        "def my_preprocessor(text):\n",
        "    tweets_processed = []\n",
        "    text = text.lower()\n",
        "    tokens = TweetTokenizer().tokenize(text)\n",
        "    handle_pattern = re.compile(r'@\\w+')  # Remove Twitter handles\n",
        "    url_pattern = re.compile(r'http[s]?://\\S+|www\\.\\S+')  # Remove URLs\n",
        "    tokens_cleaned = [token for token in tokens if not handle_pattern.match(token) and not url_pattern.match(token)]\n",
        "    tokens_no_punc = [token for token in tokens_cleaned if token not in string.punctuation]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens_no_punc]\n",
        "    tokens_wo_stopword = [token for token in lemmatized_tokens if token not in stop_words]\n",
        "    text_preprocessed = ' '.join(tokens_wo_stopword)\n",
        "    return text_preprocessed\n",
        "\n",
        "#sample data\n",
        "sample_size_per_class = 50000\n",
        "pos_samples = tw_df[tw_df['target'] == 4].sample(sample_size_per_class, random_state=2)\n",
        "neg_samples = tw_df[tw_df['target'] == 0].sample(sample_size_per_class, random_state=2)\n",
        "\n",
        "all_samples = pd.concat([pos_samples, neg_samples])\n",
        "all_samples['target'].value_counts()\n",
        "\n",
        "#train and test split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_samples['text'], all_samples['target'], test_size=0.2, random_state=2)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000, preprocessor= my_preprocessor)\n",
        "\n",
        "# a. Train the TF-IDF Vectorizer using the training data,\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# b. Apply the vectorizer to both the training and testing data\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# c. Convert the sparse matrix from the vectorization into an array matrix\n",
        "X_train_vectorized = X_train_tfidf.toarray()\n",
        "X_test_vectorized = X_test_tfidf.toarray()\n",
        "\n",
        "\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = classifier.predict(X_test_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe-200-Twitter with Logistic Regression"
      ],
      "metadata": {
        "id": "1CNPk-XfoBjM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhncR9RYi1YH",
        "outputId": "4b4ba80e-ce41-4005-ceeb-6081d692d43f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "[==================================================] 100.0% 758.5/758.5MB downloaded\n",
            "Logistic Regression Accuracy: 0.7441\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gensim.downloader as api\n",
        "from google.colab import drive\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Load Twitter dataset\n",
        "tw_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/My Folder/tweeter_training (3).csv', encoding='ISO-8859-1', header=None)\n",
        "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
        "tw_df.columns = column_names\n",
        "\n",
        "# Initialize stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def my_preprocessor(text):\n",
        "    tweets_processed = []\n",
        "    text = text.lower()\n",
        "    tokens = TweetTokenizer().tokenize(text)\n",
        "    handle_pattern = re.compile(r'@\\w+')  # Remove Twitter handles\n",
        "    url_pattern = re.compile(r'http[s]?://\\S+|www\\.\\S+')  # Remove URLs\n",
        "    tokens_cleaned = [token for token in tokens if not handle_pattern.match(token) and not url_pattern.match(token)]\n",
        "    tokens_no_punc = [token for token in tokens_cleaned if token not in string.punctuation]\n",
        "    tokens_wo_stopword = [token for token in tokens_no_punc if token not in stop_words]\n",
        "    text_preprocessed = ' '.join(tokens_wo_stopword)\n",
        "    return text_preprocessed\n",
        "\n",
        "# Preprocess tweets\n",
        "tw_df['processed_text'] = tw_df['text'].apply(my_preprocessor)\n",
        "\n",
        "# Sample dataset\n",
        "sample_size_per_class = 50000\n",
        "pos_samples = tw_df[tw_df['target'] == 4].sample(sample_size_per_class, random_state=2)\n",
        "neg_samples = tw_df[tw_df['target'] == 0].sample(sample_size_per_class, random_state=2)\n",
        "all_samples = pd.concat([pos_samples, neg_samples])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_samples['processed_text'], all_samples['target'], test_size=0.2, random_state=2)\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "word_embedding_model = api.load(\"glove-twitter-200\")\n",
        "\n",
        "# Function to compute average GloVe embeddings\n",
        "def get_embeddings_avg(texts, model):\n",
        "    texts_embedding_avg = []\n",
        "    for text in texts:\n",
        "        tokens = word_tokenize(text)\n",
        "        word_vectors = [model[word] for word in tokens if word in model.key_to_index]\n",
        "        text_embedding_avg = np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)\n",
        "        texts_embedding_avg.append(text_embedding_avg)\n",
        "    return np.array(texts_embedding_avg)\n",
        "\n",
        "# Compute GloVe embeddings for train and test sets\n",
        "X_train_glove = get_embeddings_avg(X_train, word_embedding_model)\n",
        "X_test_glove = get_embeddings_avg(X_test, word_embedding_model)\n",
        "\n",
        "# Train logistic regression model\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train_glove, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = classifier.predict(X_test_glove)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF with Random Forest"
      ],
      "metadata": {
        "id": "KgYvnk_NoHUE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKhtaq2dTTYb",
        "outputId": "f4f90c42-0b90-47ca-c3a1-9bb7fe794860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Random Forest Performance:\n",
            "Accuracy: 0.74495\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "import string\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tkinter.constants import X\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "tw_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/My Folder/tweeter_training (3).csv', header=None, encoding=\"ISO-8859-1\")\n",
        "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
        "tw_df.columns = column_names\n",
        "tweet_column_index = 5\n",
        "\n",
        "\n",
        "# Preprocessing function with lemmatization\n",
        "def my_preprocessor(text):\n",
        "    tweets_processed = []\n",
        "    text = text.lower()\n",
        "    tokens = TweetTokenizer().tokenize(text)\n",
        "    handle_pattern = re.compile(r'@\\w+')  # Remove Twitter handles\n",
        "    url_pattern = re.compile(r'http[s]?://\\S+|www\\.\\S+')  # Remove URLs\n",
        "    tokens_cleaned = [token for token in tokens if not handle_pattern.match(token) and not url_pattern.match(token)]\n",
        "    tokens_no_punc = [token for token in tokens_cleaned if token not in string.punctuation]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens_no_punc]\n",
        "    tokens_wo_stopword = [token for token in lemmatized_tokens if token not in stop_words]\n",
        "    text_preprocessed = ' '.join(tokens_wo_stopword)\n",
        "    return text_preprocessed\n",
        "\n",
        "#sample data\n",
        "sample_size_per_class = 50000\n",
        "pos_samples = tw_df[tw_df['target'] == 4].sample(sample_size_per_class, random_state=2)\n",
        "neg_samples = tw_df[tw_df['target'] == 0].sample(sample_size_per_class, random_state=2)\n",
        "\n",
        "all_samples = pd.concat([pos_samples, neg_samples])\n",
        "all_samples['target'].value_counts()\n",
        "\n",
        "#train and test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_samples['text'], all_samples['target'], test_size=0.2, random_state=2)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000, preprocessor= my_preprocessor)\n",
        "\n",
        "# a. Train the TF-IDF Vectorizer using the training data,\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# b. Apply the vectorizer to both the training and testing data\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Random Forest Classifier model\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict using Random Forest\n",
        "y_pred_rf = rf_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate Model\n",
        "print(\"Random Forest Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe-200-Twitter with Random Forest"
      ],
      "metadata": {
        "id": "PggDTAgfoKW9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nn0u9TzYPqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3acb91-80d7-4f8c-bb37-908eeff85d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Random Forest Performance:\n",
            "Accuracy: 0.7238\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gensim.downloader as api\n",
        "from google.colab import drive\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Load Twitter dataset\n",
        "tw_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/My Folder/tweeter_training (3).csv', encoding='ISO-8859-1', header=None)\n",
        "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
        "tw_df.columns = column_names\n",
        "\n",
        "# Initialize stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def my_preprocessor(text):\n",
        "    tweets_processed = []\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize().tokenize(text)\n",
        "    handle_pattern = re.compile(r'@\\w+')  # Remove Twitter handles\n",
        "    url_pattern = re.compile(r'http[s]?://\\S+|www\\.\\S+')  # Remove URLs\n",
        "    tokens_cleaned = [token for token in tokens if not handle_pattern.match(token) and not url_pattern.match(token)]\n",
        "    tokens_no_punc = [token for token in tokens_cleaned if token not in string.punctuation]\n",
        "    tokens_wo_stopword = [token for token in tokens_no_punc if token not in stop_words]\n",
        "    text_preprocessed = ' '.join(tokens_wo_stopword)\n",
        "    return text_preprocessed\n",
        "\n",
        "# Preprocess tweets\n",
        "tw_df['processed_text'] = tw_df['text'].apply(my_preprocessor)\n",
        "\n",
        "# Sample dataset\n",
        "sample_size_per_class = 50000\n",
        "pos_samples = tw_df[tw_df['target'] == 4].sample(sample_size_per_class, random_state=2)\n",
        "neg_samples = tw_df[tw_df['target'] == 0].sample(sample_size_per_class, random_state=2)\n",
        "all_samples = pd.concat([pos_samples, neg_samples])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_samples['processed_text'], all_samples['target'], test_size=0.2, random_state=2)\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "word_embedding_model = api.load(\"glove-twitter-200\")\n",
        "\n",
        "# Function to compute average GloVe embeddings\n",
        "def get_embeddings_avg(texts, model):\n",
        "    texts_embedding_avg = []\n",
        "    for text in texts:\n",
        "        tokens = word_tokenize(text)\n",
        "        word_vectors = [model[word] for word in tokens if word in model.key_to_index]\n",
        "        text_embedding_avg = np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)\n",
        "        texts_embedding_avg.append(text_embedding_avg)\n",
        "    return np.array(texts_embedding_avg)\n",
        "\n",
        "# Compute GloVe embeddings for train and test sets\n",
        "X_train_glove = get_embeddings_avg(X_train, word_embedding_model)\n",
        "X_test_glove = get_embeddings_avg(X_test, word_embedding_model)\n",
        "\n",
        "# Train Random Forest Classifier model\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train_glove, y_train)\n",
        "\n",
        "# Predict using Random Forest\n",
        "y_pred_rf = rf_classifier.predict(X_test_glove)\n",
        "\n",
        "# Evaluate Model\n",
        "print(\"Random Forest Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}